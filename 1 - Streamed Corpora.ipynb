{
 "metadata": {
  "name": "",
  "signature": "sha256:ca397897f5d477a9d854e25343769b43ab92c15bb3a70a4c66438bbaab9124b3"
 },
 "nbformat": 3,
 "nbformat_minor": 0,
 "worksheets": [
  {
   "cells": [
    {
     "cell_type": "heading",
     "level": 1,
     "metadata": {},
     "source": [
      "Topic Modeling for Fun and Profit"
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "In this notebook:\n",
      "\n",
      "* you learn efficient patterns for processing large corpora (streamed processing)\n",
      "* I try to convince you iterators and generators are a useful (and joyful!) tool, not black magic\n",
      "* you write your own streamed text processing pipeline, incl. basic NLP: collocation detection, lemmatization, stopwords..."
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "# import and setup modules we'll be using in this notebook\n",
      "import logging\n",
      "import os\n",
      "import sys\n",
      "import re\n",
      "import tarfile\n",
      "import itertools\n",
      "\n",
      "import nltk\n",
      "from nltk.collocations import TrigramCollocationFinder\n",
      "from nltk.metrics import BigramAssocMeasures, TrigramAssocMeasures\n",
      "\n",
      "import gensim\n",
      "from gensim.parsing.preprocessing import STOPWORDS\n",
      "\n",
      "logging.basicConfig(format='%(levelname)s : %(message)s', level=logging.INFO)\n",
      "logging.root.level = logging.INFO  # ipython sometimes messes up the logging setup; restore"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "heading",
     "level": 2,
     "metadata": {},
     "source": [
      "Data preprocessing"
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "Previously, we have downloaded the 20newsgroups dataset and left it under `./data/` as a zipped tarball:"
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "!ls -l ./data/20news-bydate.tar.gz"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "Let's use this file for some topic modeling. Instead of decompressing its files, let's access them directly from Python:"
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "with tarfile.open('./data/20news-bydate.tar.gz', 'r:gz') as tf:\n",
      "    # get information (metadata) about all files in the tarball\n",
      "    file_infos = [file_info for file_info in tf if file_info.isfile()]\n",
      "    \n",
      "    # print one of them; for example, the last one\n",
      "    message = tf.extractfile(file_infos[0]).read()\n",
      "    print(message)"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "This text is typical of real-world data. It contains a mix of relevant text, metadata (email headers), and downright noise. Even its relevant content is unstructured, with email addresses, people's names, quotations etc.\n",
      "\n",
      "Most machine learning methods, topic modeling included, are only as good as the data you give it. At this point, we generally want to clean the data as much as possible. While the subsequent steps in the machine learning pipeline are more or less automated, handling the raw data should reflect the intended purpose of the application, its business logic, idiosyncracies, sanity check (aren't we accidentally receiving and parsing *image data instead of plain text*?). As always with automated processing it's [garbage in, garbage out](http://en.wikipedia.org/wiki/Garbage_in,_garbage_out)."
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "As an example, let's write a function that aims to extract only the chunk of relevant text, ignoring the email headers:"
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "def process_message(message):\n",
      "    \"\"\"\n",
      "    Preprocess a single 20newsgroups message, returning the result as\n",
      "    a unicode string.\n",
      "    \n",
      "    \"\"\"\n",
      "    message = gensim.utils.to_unicode(message, 'latin1').strip()\n",
      "    blocks = message.split(u'\\n\\n')\n",
      "    # skip email headers (first block) and footer (last block)\n",
      "    content = u'\\n\\n'.join(blocks[1:-1])\n",
      "    return content\n",
      "\n",
      "print process_message(message)"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "Feel free to modify this function and test out other ideas for clean up. The flexibility Python gives you in processing text is superb -- [it'd be a crime](http://radimrehurek.com/2014/03/data-streaming-in-python-generators-iterators-iterables/) to hide the processing behind opaque APIs, exposing only one or two tweakable parameters.\n",
      "\n",
      "There are a handful of handy Python libraries for text cleanup: [jusText](https://github.com/miso-belica/jusText) removes HTML boilerplate and extracting \"main text\". FIXME NLTK, Pattern and TextBlob for tokenization and generic NLP. Tokenization, sentence splitting, chunking, POS tagging."
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "**Exercise (10 min)**: FIXME modify somehow."
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "It's a good practice to inspect your data visually, at each point as it passes through your data processing pipeline. Simple printing (logging) a few arbitrary entries, ala UNIX `head`, does wonders for spotting unexpected bugs. *Oh, bad encoding! What is Chinese doing there, we were told all texts are English only? Do these rubbish tokens come from embedded images? How come everything's empty?* Taking a text with \"hey, let's tokenize it into a bag of words blindly, like they do in the tutorials, push it through this magical unicorn ML library and hope for the best\" is ill advised.\n",
      "\n",
      "Another good practice is to keep internal strings as Unicode, and only encode/decode on IO (preferably using UTF8). As of Python 3.3, there is practically no memory penalty for using decoded Unicode ([PEP 393](http://legacy.python.org/dev/peps/pep-0393/))."
     ]
    },
    {
     "cell_type": "heading",
     "level": 2,
     "metadata": {},
     "source": [
      "Data streaming"
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "\n",
      "Let's write a function to go over all messages in the 20newsgroups archive:"
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "def iter_20newsgroups(fname, log_every=None):\n",
      "    \"\"\"\n",
      "    Yield plain text of each 20 newsgroups message, as a unicode string.\n",
      "\n",
      "    The messages are read from raw tar.gz file `fname` on disk (e.g. `./data/20news-bydate.tar.gz`)\n",
      "\n",
      "    \"\"\"\n",
      "    extracted = 0\n",
      "    with tarfile.open(fname, 'r:gz') as tf:\n",
      "        for file_number, file_info in enumerate(tf):\n",
      "            if file_info.isfile():\n",
      "                if log_every and extracted % log_every == 0:\n",
      "                    logging.info(\"extracting 20newsgroups file #%i: %s\" % (extracted, file_info.name))\n",
      "                content = tf.extractfile(file_info).read()\n",
      "                yield process_message(content)\n",
      "                extracted += 1"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "This uses the `process_message()` we wrote above, to process each message in turn. The messages are extracted on-the-fly, one after another, using a generator.\n",
      "\n",
      "Such **data streaming** is a very important pattern: real data is typically too large to fit into RAM, and we don't need all of it in RAM at the same time anyway -- that's just wasteful. With streamed data, we can process arbitrarily large input, reading the data from a file on disk, shared network disk, or even more exotic remote network protocols."
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "# itertools is an inseparable friend with data streaming (Python built-in library)\n",
      "import itertools\n",
      "\n",
      "# let's only parse and print the first three messages, lazily\n",
      "# `list(stream)` materializes the stream elements into a plain Python list\n",
      "message_stream = iter_20newsgroups('./data/20news-bydate.tar.gz', log_every=2)\n",
      "print(list(itertools.islice(message_stream, 3)))"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "**Note:** Data streaming saves memory, but does nothing to save time. A common pattern for speeding up data processing is parallelization, via Python's multi-processing and multi-threading support. We don't have time to cover processing parallelization or cluster distribution in this tutorial. For an example, see [this Wikipedia parsing code](https://github.com/piskvorky/sim-shootout/blob/master/prepare_shootout.py#L48) from my [Similarity Shootout benchmark](http://radimrehurek.com/2013/12/performance-shootout-of-nearest-neighbours-contestants/)."
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "A generator only gives us a single pass through the data:"
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "# print the next two messages; the three messages printed above are already gone\n",
      "print(list(itertools.islice(message_stream, 2)))"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "Let's wrap the generator inside an object's `__iter__` method, so we can iterate over the stream multiple times:"
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "class Corpus20News(object):\n",
      "    def __init__(self, fname):\n",
      "        self.fname = fname\n",
      "\n",
      "    def __iter__(self):\n",
      "        for text in iter_20newsgroups(self.fname):\n",
      "            # tokenize each message; simply lowercase & match alphabetic chars, for now\n",
      "            yield list(gensim.utils.tokenize(text, lower=True))\n",
      "\n",
      "tokenized_corpus = Corpus20News('./data/20news-bydate.tar.gz')\n",
      "\n",
      "# print the first two tokenized messages\n",
      "print(list(itertools.islice(tokenized_corpus, 2)))"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "# the same two tokenized messages (not the next two!)\n",
      "# each call to __iter__ \"resets\" the stream, by creating a new generator object internally\n",
      "print(list(itertools.islice(tokenized_corpus, 2)))"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "heading",
     "level": 2,
     "metadata": {},
     "source": [
      "Text processing"
     ]
    },
    {
     "cell_type": "heading",
     "level": 3,
     "metadata": {},
     "source": [
      "Lemmatization, stemming"
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "[Lemmatization](http://en.wikipedia.org/wiki/Lemmatisation) is type of normalization that treats different inflected forms of a word as a single unit (\"work\", \"working\", \"works\", \"worked\", \"working\" => same lemma: \"work\"):"
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "import gensim\n",
      "\n",
      "print(gensim.utils.lemmatize(\"worked\"))\n",
      "print(gensim.utils.lemmatize(\"working\"))\n",
      "print(gensim.utils.lemmatize(\"The big fat cows jumped over a quick brown foxes.\"))"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "There's a part of speech (POS) tag included in each token: lemma/POS. Note how articles and prepositions, such as \"The\", \"a\", or \"over\", have been filtered out from the result. Only word categories that traditionally carry the most meaning, such as nouns, adjectives and verbs, are left. Gensim uses the [pattern](http://www.clips.ua.ac.be/pattern) library internally, because its lemmatization performs (much) better than alternatives such as NLTK."
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "class Corpus20News_Lemmatize(object):\n",
      "    def __init__(self, fname):\n",
      "        self.fname = fname\n",
      "\n",
      "    def __iter__(self):\n",
      "        for message in iter_20newsgroups(self.fname):\n",
      "            yield self.tokenize(message)\n",
      "\n",
      "    def tokenize(self, text):\n",
      "        \"\"\"Break text into a list of lemmatized words.\"\"\"\n",
      "        return gensim.utils.lemmatize(text)\n",
      "    \n",
      "lemmatized_corpus = Corpus20News_Lemmatize('./data/20news-bydate.tar.gz')\n",
      "print(list(itertools.islice(lemmatized_corpus, 2)))"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "**Task (10 min)**: Modify `split_words()` to ignore (=not return) generic words, such as \"do\", \"then\", \"be\". These are called stopwords and we may want to remove them because some topic modeling algorithms are sensitive to their presence. An example of common stopwords for English is in `from gensim.parsing.preprocessing import STOPWORDS`."
     ]
    },
    {
     "cell_type": "heading",
     "level": 3,
     "metadata": {},
     "source": [
      "Collocations and Named Entity Recognition"
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "[Collocation](http://en.wikipedia.org/wiki/Collocation) is a \"sequence of words or terms that co-occur more often than would be expected by chance.\"\n",
      "\n",
      "[Named entity recognition (NER)](http://en.wikipedia.org/wiki/Named-entity_recognition) is the task of locating chunks of text that refer to people, locations, organizations etc.\n",
      "\n",
      "Detecting collocations and named entities often has a significant business value: \"General Electric\" stays a single entity (token), rather than two words \"general\" and \"electric\". Same with \"Marathon Petroleum\", \"George Bush\" etc -- the model doesn't confuse , both clearer topics FIXME."
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "import nltk\n",
      "from nltk.collocations import TrigramCollocationFinder\n",
      "from nltk.metrics import BigramAssocMeasures, TrigramAssocMeasures\n",
      "\n",
      "def best_ngrams(words, top_n=1000, min_freq=100):\n",
      "    \"\"\"\n",
      "    Extract `top_n` most salient collocations (bigrams and trigrams),\n",
      "    from a stream of words. Ignore collocations with frequency\n",
      "    lower than `min_freq`.\n",
      "\n",
      "    This fnc uses NLTK for the collocation detection itself -- not very scalable!\n",
      "\n",
      "    Return the detected ngrams as compiled regular expressions, for their faster\n",
      "    detection later on.\n",
      "\n",
      "    \"\"\"\n",
      "    tcf = TrigramCollocationFinder.from_words(words)\n",
      "    tcf.apply_freq_filter(min_freq)\n",
      "    trigrams = [' '.join(w) for w in tcf.nbest(TrigramAssocMeasures.chi_sq, top_n)]\n",
      "    logging.info(\"%i trigrams found: %s...\" % (len(trigrams), list(trigrams)[:20]))\n",
      "\n",
      "    bcf = tcf.bigram_finder()\n",
      "    bcf.apply_freq_filter(min_freq)\n",
      "    bigrams = [' '.join(w) for w in bcf.nbest(BigramAssocMeasures.pmi, top_n)]\n",
      "    logging.info(\"%i bigrams found: %s...\" % (len(bigrams), list(bigrams)[:20]))\n",
      "\n",
      "    pat_gram2 = re.compile('(%s)' % '|'.join(bigrams), re.UNICODE)\n",
      "    pat_gram3 = re.compile('(%s)' % '|'.join(trigrams), re.UNICODE)\n",
      "\n",
      "    return pat_gram2, pat_gram3"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "from gensim.parsing.preprocessing import STOPWORDS\n",
      "\n",
      "class Corpus20News_Collocations(object):\n",
      "    def __init__(self, fname):\n",
      "        self.fname = fname\n",
      "        logging.info(\"collecting ngrams from %s\" % self.fname)\n",
      "        documents = (self.split_words(text) for text in iter_20newsgroups(self.fname, log_every=1000))\n",
      "        words = itertools.chain.from_iterable(documents)\n",
      "        self.bigrams, self.trigrams = best_ngrams(words)\n",
      "\n",
      "    def __iter__(self):\n",
      "        for message in iter_20newsgroups(self.fname):\n",
      "            yield self.tokenize(message)\n",
      "\n",
      "    def tokenize(self, message):\n",
      "        \"\"\"\n",
      "        Break text (string) into a list of Unicode tokens.\n",
      "        \n",
      "        The resulting tokens can be longer phrases (collocations) too,\n",
      "        e.g. `new_york`, `real_estate` etc.\n",
      "\n",
      "        \"\"\"\n",
      "        text = u' '.join(self.split_words(message))\n",
      "        text = re.sub(self.trigrams, lambda match: match.group(0).replace(u' ', u'_'), text)\n",
      "        text = re.sub(self.bigrams, lambda match: match.group(0).replace(u' ', u'_'), text)\n",
      "        return text.split()\n",
      "\n",
      "    def split_words(self, text, stopwords=STOPWORDS):\n",
      "        \"\"\"\n",
      "        Break text into a list of single words. Ignore any token that falls into\n",
      "        the `stopwords` set.\n",
      "\n",
      "        \"\"\"\n",
      "        return [word\n",
      "                for word in gensim.utils.tokenize(text, lower=True)\n",
      "                if word not in STOPWORDS and len(word) > 3]\n",
      "\n",
      "%time collocations_corpus = Corpus20News_Collocations('./data/20news-bydate.tar.gz')\n",
      "print(list(itertools.islice(collocations_corpus, 2)))"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "Instead of detecting collocations by frequency, we can run (shallow) syntactic parsing. This tags each word with its part-of-speech (POS) category, and suggests phrases based on chunks of \"noun phrases\":"
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "from textblob import TextBlob\n",
      "\n",
      "def head(stream, n=10):\n",
      "    \"\"\"Convenience fnc: return the first `n` elements of the stream, as plain list.\"\"\"\n",
      "    return list(itertools.islice(stream, n))\n",
      "\n",
      "def best_phrases(document_stream, top_n=1000, prune_at=100000):\n",
      "    \"\"\"Return a set of `top_n` most common noun phrases.\"\"\"\n",
      "    np_counts = {}\n",
      "    for docno, doc in enumerate(document_stream):\n",
      "        # prune out infrequent phrases from time to time, to save RAM.\n",
      "        # the result may not be completely accurate because of this step\n",
      "        if docno % 1000 == 0:\n",
      "            sorted_phrases = sorted(np_counts.iteritems(), key=lambda item: -item[1])\n",
      "            np_counts = dict(sorted_phrases[:prune_at])\n",
      "            logging.info(\"at document #%i, considering %i phrases: %s...\" %\n",
      "                         (docno, len(np_counts), head(sorted_phrases)))\n",
      "        \n",
      "        # how many times have we seen each noun phrase?\n",
      "        for np in TextBlob(doc).noun_phrases:\n",
      "            # only consider multi-word NEs where each word contains at least one letter\n",
      "            if u' ' not in np:\n",
      "                continue\n",
      "            # ignore phrases that contains too short/non-alphanetic words\n",
      "            if all(len([ch for ch in word if ch.isalpha()]) > 2 for word in np.split()):\n",
      "                np_counts[np] = np_counts.get(np, 0) + 1\n",
      "\n",
      "    sorted_phrases = sorted(np_counts, key=lambda np: -np_counts[np])\n",
      "    return set(head(sorted_phrases, top_n))"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "class Corpus20News_NE(object):\n",
      "    def __init__(self, fname):\n",
      "        self.fname = fname\n",
      "        logging.info(\"collecting entities from %s\" % self.fname)\n",
      "        doc_stream = itertools.islice(iter_20newsgroups(self.fname), 10000)\n",
      "        self.entities = best_phrases(doc_stream)\n",
      "        logging.info(\"selected %i entities: %s...\" %\n",
      "                     (len(self.entities), list(self.entities)[:10]))\n",
      "\n",
      "    def __iter__(self):\n",
      "        for message in iter_20newsgroups(self.fname):\n",
      "            yield self.tokenize(message)\n",
      "\n",
      "    def tokenize(self, message, stopwords=STOPWORDS):\n",
      "        \"\"\"\n",
      "        Break text (string) into a list of Unicode tokens.\n",
      "        \n",
      "        The resulting tokens can be longer phrases (named entities) too,\n",
      "        e.g. `new_york`, `real_estate` etc.\n",
      "\n",
      "        \"\"\"\n",
      "        result = []\n",
      "        for np in TextBlob(message).noun_phrases:\n",
      "            if u' ' in np and np not in self.entities:\n",
      "                # only consider multi-word phrases we detected in the constructor\n",
      "                continue\n",
      "            token = u'_'.join(part for part in gensim.utils.tokenize(np) if len(part) > 2)\n",
      "            if len(token) < 4 or token in stopwords:\n",
      "                # ignore very short phrases and stop words\n",
      "                continue\n",
      "            result.append(token)\n",
      "        return result\n",
      "\n",
      "%time ne_corpus = Corpus20News_NE('./data/20news-bydate.tar.gz')\n",
      "print(head(ne_corpus, 5))"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "**Exercise**: FIXME"
     ]
    },
    {
     "cell_type": "heading",
     "level": 2,
     "metadata": {},
     "source": [
      "Too much work?"
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "What we've just done, cleaning up raw input as the first step to more advanced processing, is actually the most \"varied\" and the most challenging part of building up machine learning pipelines (along with the last step, evaluation).\n",
      "\n",
      "You typically have to know what overall goal you're trying to achieve to choose the correct approach. There is no \"one best way\" to preprocess text -- different applications require different preprocessing steps, all the way down to custom tokenizers and lemmatizers. This is especially true for other (non-English) languages. Always log liberally and check the output coming out of your pipeline at various steps, to spot potential unforeseen problems.\n",
      "\n",
      "Now that we have the data in a common format, ready to be vectorized, subsequent notebooks will be more straightforward & run of the mill."
     ]
    },
    {
     "cell_type": "heading",
     "level": 2,
     "metadata": {},
     "source": [
      "Summary"
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "Flow of data preparation:\n",
      "\n",
      "* extract a text stream from (raw) data\n",
      "* clean up texts, depending on business logic\n",
      "* break sanitized text into features of interest: words, collocations, detect named entities...\n",
      "* keep the data flow streamed and flexible\n",
      "* sprinkle code with sanity prints&checks (DEBUG/INFO logs)"
     ]
    },
    {
     "cell_type": "heading",
     "level": 2,
     "metadata": {},
     "source": [
      "Next"
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "In the next notebook, we'll learn how to plug such preprocessed data streams into gensim, a library for topic modeling and information retrieval.\n",
      "\n",
      "Continue with opening the next ipython notebook, `2 - Topic Modeling`."
     ]
    }
   ],
   "metadata": {}
  }
 ]
}